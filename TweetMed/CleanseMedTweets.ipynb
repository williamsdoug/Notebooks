{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cleanse Medical Symptom related tweets\n",
    "\n",
    "###Updated 6/4/15\n",
    "\n",
    "Steps:\n",
    "- Load list of drugs [done]\n",
    "- Filter tweets by reference to drug [done]\n",
    "- Add spam filter - tweets with large amount of common content [done]\n",
    "- Apply Topic Modeling and Clustering [See separate notebook]\n",
    "- Additional filter to determine relevance\n",
    "- Sentiment analysis of selected tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Tweet Format\n",
    "\n",
    "```python\n",
    "[u'contributors',\n",
    " u'truncated',\n",
    " u'text',\n",
    " u'in_reply_to_status_id',\n",
    " u'id',\n",
    " u'favorite_count',\n",
    " u'source',\n",
    " u'retweeted',\n",
    " u'coordinates',\n",
    " u'timestamp_ms',\n",
    " u'entities',\n",
    " u'in_reply_to_screen_name',\n",
    " u'id_str',\n",
    " u'retweet_count',\n",
    " u'in_reply_to_user_id',\n",
    " u'favorited',\n",
    " u'retweeted_status',\n",
    " u'user',\n",
    " u'geo',\n",
    " u'in_reply_to_user_id_str',\n",
    " u'possibly_sensitive',\n",
    " u'lang',\n",
    " u'created_at',\n",
    " u'filter_level',\n",
    " u'in_reply_to_status_id_str',\n",
    " u'place',\n",
    " u'extended_entities']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pattern.en import parse\n",
    "from pattern.en import ngrams\n",
    "from pattern.en.wordlist import BASIC as BASIC_WORDS\n",
    "# import lshash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_PATH = './Corpus/'\n",
    "MAX_DRUG_NGRAM = 3\n",
    "HASH_LEN = 5\n",
    "HASH_BITS = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore = [\n",
    "    'camila',\n",
    "    'nikki',\n",
    "    'testosterone',\n",
    "    'ella',\n",
    "    'fml',\n",
    "    'muse', \n",
    "    'viagra',\n",
    "    'talc',\n",
    "    'bal',\n",
    "    'nicotine',\n",
    "    'heather',\n",
    "    'plan',\n",
    "    'alcohol',\n",
    "    'vitamin',\n",
    "    'metro',\n",
    "    'android',\n",
    "    'calcium',\n",
    "    'capital',\n",
    "    'xenon',\n",
    "    'lo', \n",
    "    'sodium',\n",
    "    'amino',\n",
    "    'caffeine',\n",
    "    'sterile',\n",
    "    'extended',\n",
    "    'aspirin',\n",
    "    'ssd',\n",
    "    'potassium',\n",
    "    'olive',\n",
    "    'balanced',\n",
    "    'magnesium',\n",
    "    'flavored',\n",
    "    'peg', \n",
    "    'ammonia',\n",
    "    'ms',\n",
    "    'soybean',\n",
    "    'zinc',\n",
    "    'tobi',\n",
    "    'capex',\n",
    "    'sulfur',\n",
    "    'sps',\n",
    "    'folic',\n",
    "    'citric',\n",
    "    'arsenic',\n",
    "    'ibuprofen',\n",
    "    'tylenol',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Filter Tweets based on reference to drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_tweets(directory, extract=lambda x:json.loads(x)):\n",
    "    \"\"\"Generator function to return all tweets in a zipped firectory\"\"\"\n",
    "    for fn in os.listdir(directory):\n",
    "        if fn.endswith('.json.gz'):\n",
    "            try:\n",
    "                f = gzip.open(directory+fn, 'rb')\n",
    "                for line in f:\n",
    "                    yield extract(line)\n",
    "                f.close()\n",
    "            except IOError:\n",
    "                print 'IOError for file {}'.format(fn)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def myfilter(x):\n",
    "    \"\"\"Filters fields within tweet to reduce size\"\"\"\n",
    "    x = json.loads(x)\n",
    "    result={}\n",
    "    if 'delete' not in x:\n",
    "        \n",
    "        if 'text' in x:\n",
    "            result['text'] = x['text']\n",
    "        else:\n",
    "            False\n",
    "        if 'id_str' in x:\n",
    "            result['id_str'] = x['id_str']\n",
    "        if 'user' in x and 'screen_name' in x['user']:\n",
    "            result['screen_name'] = x['user']['screen_name']\n",
    "        #result['name'] = x['user']['name']\n",
    "        if 'entities' in x:\n",
    "            result['user_mentions'] = [men['screen_name'].lower() for men in x['entities']['user_mentions']]\n",
    "            result['hashtags'] = [entry['text'].lower() for entry in x['entities']['hashtags']]\n",
    "        if 'lang' in x:\n",
    "            result['lang'] = x['lang']\n",
    "        return result\n",
    "    else:\n",
    "        False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test routine for all_tweets\n",
    "def test_load_tweet(dir_path=DIR_PATH):\n",
    "    \"\"\"Debug routine, should be deletd in future version\"\"\"\n",
    "    all_count = 0\n",
    "    count = 0\n",
    "    has_text = 0\n",
    "    users = collections.defaultdict(int)\n",
    "    mentions = collections.defaultdict(int)\n",
    "    hashtags = collections.defaultdict(int)\n",
    "    try:\n",
    "        for x in all_tweets(dir_path, extract=myfilter):\n",
    "            if x:\n",
    "                all_count += 1\n",
    "            else:\n",
    "                continue\n",
    "            if 'text' in x:\n",
    "                has_text += 1\n",
    "            if x and x['lang']=='en':\n",
    "                count += 1\n",
    "                users[x['screen_name'].lower()] += 1\n",
    "                for tag in x['hashtags']:\n",
    "                    #pprint.pprint(tag)\n",
    "                    hashtags[tag] += 1\n",
    "                for men in x['user_mentions']:\n",
    "                    mentions[men] += 1\n",
    "    except IOError:\n",
    "        print 'IOError'\n",
    "        print 'foobar'\n",
    "        pass\n",
    "    print 'total tweets:', all_count\n",
    "    print 'total tweets with text:', has_text\n",
    "    print 'total english tweets: ', count\n",
    "    print 'unique users: ',len(users)\n",
    "    print 'unique mentions: ',len(mentions)\n",
    "    print 'unique hashtags: ',len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_tweet(dir_path=DIR_PATH, progress=10000):\n",
    "    \"\"\"Loads corpus of tweets, where corpus is composed of a set of\n",
    "    xxx.json.gz files within a specified directory\"\"\"\n",
    "    result = []\n",
    "\n",
    "    for i, x in enumerate(all_tweets(dir_path, extract=myfilter)):\n",
    "        if i%(progress*10) == 0 and i > 0:\n",
    "            print i,\n",
    "            sys.stdout.flush()\n",
    "        elif i%progress == 0 and i > 0:\n",
    "            print '*',\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        if not x:\n",
    "            continue\n",
    "        if 'text' not in x:\n",
    "            continue\n",
    "        if x and x['lang']=='en':\n",
    "            result.append(x)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createDrugLookupTable(fname):\n",
    "    \"\"\"Converts drug list to set for performing token lookups\n",
    "    For ngrame names, only use the first word in name\"\"\"\n",
    "\n",
    "    drug_list = pickle.load(open(fname, \"rb\" ))\n",
    "    idx_drugs = {name.strip().lower().split()[0] for name in drug_list}\n",
    "    # Remove drug names that are also part of the basic english language\n",
    "    idx_drugs = idx_drugs.difference({ w.lower() for w in BASIC_WORDS})\n",
    "    print 'Total Unique Drug Name Prefixes: {0} in file {1}'.format(len(idx_drugs), fname)\n",
    "    return idx_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterToken(tok):\n",
    "    \"\"\"Removes @ and # from token\"\"\"\n",
    "    return tok[1:] if (tok.startswith('#') or tok.startswith('@')) else tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filterTweetCorpus(tweet_corpus, drugs, ignore=[], progress=10000, ignore_retweet=True):\n",
    "    \"\"\"Top level routine to filter tweets based on presence of drug name\n",
    "    ngram drug name lookup currently disabled (low hit rate, improves speed)\"\"\"\n",
    "    counts = collections.defaultdict(int)\n",
    "    results = []\n",
    "    drugs_subset = drugs.difference(set(ignore))  # filter out ignored drugs\n",
    "    for tweetno, t in enumerate(tweet_corpus):\n",
    "        if tweetno%(progress*10) == 0 and tweetno > 0:\n",
    "            print tweetno,\n",
    "            sys.stdout.flush()\n",
    "        elif tweetno%progress == 0 and tweetno > 0:\n",
    "            print '*',\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        txt = t['text'].lower()\n",
    "    \n",
    "        if ignore_retweet and txt.startswith('rt'):  # ignore retweets\n",
    "            continue\n",
    "        \n",
    "        # drug names\n",
    "        all_tokens = {filterToken(one_gram[0]) for one_gram in ngrams(txt, n=1)}\n",
    "        drug_ref = all_tokens.intersection(drugs_subset)\n",
    "        if drug_ref:\n",
    "            results.append({'drugs':list(drug_ref), 'text':txt})\n",
    "            # print '**', list(drug_ref), '**', txt\n",
    "            # print\n",
    "            for tok in drug_ref:\n",
    "                counts[tok] += 1\n",
    "\n",
    "    return results, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: http://primes.utm.edu/lists/small/10000.txt\n",
    "# Largest primes where prime <= 2**n_bits\n",
    "primeTable = [\n",
    "    1, #0\n",
    "    1, #1\n",
    "    3, #2 \n",
    "    7, #3\n",
    "    13, #4\n",
    "    31, # 5\n",
    "    61, # 6\n",
    "    127, # 7\n",
    "    251, # 8\n",
    "    509, # 9\n",
    "    1021, # 10\n",
    "    2039, # 11\n",
    "    4093, # 12\n",
    "    8191, # 13\n",
    "    16381, # 14\n",
    "    32749, #15\n",
    "    65521, # 16\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adlerHashN(text, modulo=2**12):\n",
    "    \"\"\"Computes N-bit Adler hash for a text string\n",
    "    where modulo = (N-bit/2)**2\"\"\"\n",
    "    A = 1\n",
    "    B = 1\n",
    "    for c in text.lower():\n",
    "        c_val = ord(c)\n",
    "        A += c_val\n",
    "        B += A\n",
    "    return (B % modulo) * modulo + A    \n",
    "\n",
    "if False:  # Test code\n",
    "    print adlerHashN('the quick brown fox jumped over the lazy dog')\n",
    "    print adlerHashN('the quick brown fox jumped over the lazy do')\n",
    "    print adlerHashN('the quick brown cat jumped over the lazy dog')\n",
    "    print adlerHashN('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adler_lookup_or_insert(hashTable, tweet_text, tweetid, similarity_threshold=0.6,\n",
    "                         hash_len=10,  modulo=4093, verbose=False):\n",
    "    \"\"\"Use LSH to determine dedupe, return True if match exists, if not, insert\n",
    "    similarity_threshold = percentable of hashes that need to match to declare duplicate\n",
    "    \"\"\"\n",
    "\n",
    "    total_hashes = len(tweet_text) - hash_len\n",
    "    threshold = total_hashes * similarity_threshold\n",
    "    \n",
    "    # Determine whether similar tweet already exists\n",
    "    hits = 0\n",
    "    candidates = collections.defaultdict(int)\n",
    "    for i in range(total_hashes+1):\n",
    "        hashKey = adlerHashN(tweet_text[i:i+hash_len], modulo=modulo)\n",
    "        result = hashTable[hashKey]\n",
    "        if len(result) > 0:\n",
    "            hits += 1\n",
    "            for otherTweetid in result:\n",
    "                candidates[otherTweetid] += 1\n",
    "                    \n",
    "    if hits >= threshold:\n",
    "        for candidateTweetid, candidateHists in candidates.items():\n",
    "            if candidateHists >= threshold:\n",
    "                if verbose:\n",
    "                    print '{0} Found matching tweet {1}  hits: {2} threshold:{3}'.format(tweetid,\n",
    "                                                                                         candidateTweetid,\n",
    "                                                                                         candidateHists, threshold)\n",
    "                return True\n",
    "\n",
    "    \n",
    "    # Insert tweetid in  appropriate hash buckets\n",
    "    for i in range(total_hashes+1):\n",
    "        hashKey = adlerHashN(tweet_text[i:i+hash_len], modulo=modulo)\n",
    "        hashTable[hashKey] = hashTable[hashKey].union(set([tweetid]))\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dedupeTweets(tweets, hash_bits=24, hash_len=10, progress=1000, similarity_threshold=0.6, verbose=False):\n",
    "    \"\"\"Top level routine to dedupe a set of tweets using locality sensitive hashing.\n",
    "    hash_bits = number of buckets in hash table = 2**hash_bits\n",
    "    hash_len = number of characters included in each hash\n",
    "    \"\"\"\n",
    "    global primeTable\n",
    "    modulo = primeTable[int(hash_bits/2)]\n",
    "    print 'Modulo:{0}  Max: {1}'.format(modulo, 2**int(hash_bits/2))\n",
    "    hashTable = collections.defaultdict(set)\n",
    "\n",
    "    count = 0\n",
    "    results = []\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        if i and i%progress == 0:\n",
    "            print '*',\n",
    "            sys.stdout.flush()\n",
    "        # normalize text by removing URL's and balancinf whitespace\n",
    "        normalized_text = ' '.join([x for x in tweet['text'].split() if not (x.startswith('http://')\n",
    "                                                                            or x.startswith('https://'))])            \n",
    "        if adler_lookup_or_insert(hashTable, normalized_text, i,\n",
    "                                  hash_len=hash_len, modulo=modulo, similarity_threshold=similarity_threshold):\n",
    "            count += 1\n",
    "            if verbose:\n",
    "                print 'Found match:'\n",
    "                pprint(tweet)\n",
    "                print\n",
    "        else:\n",
    "            results.append(tweet)\n",
    "    print\n",
    "    print 'Total duplicate tweets:', count\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_symptoms = pickle.load(open(\"symptoms.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Drug Name Prefixes: 5297 in file all_drugs.p\n",
      "Total Unique Drug Name Prefixes: 3296 in file current_drugs.p\n",
      "Total Unique Drug Name Prefixes: 3190 in file rx_drugs.p\n",
      "Total Unique Drug Name Prefixes: 175 in file otc_drugs.p\n",
      "Total Unique Drug Name Prefixes: 3245 in file discontinued_drugs.p\n"
     ]
    }
   ],
   "source": [
    "idx_all_drugs = createDrugLookupTable('all_drugs.p')\n",
    "idx_current_drugs = createDrugLookupTable('current_drugs.p')\n",
    "idx_rx_drugs = createDrugLookupTable('rx_drugs.p')\n",
    "idx_otc_drugs = createDrugLookupTable('otc_drugs.p')\n",
    "idx_discontinued_drugs = createDrugLookupTable('discontinued_drugs.p')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_load_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * IOError for file 1433112631492061.json.gz\n",
      "* * * * * * 100000 * * * * * * * * * 200000 * * * * * * * * * 300000 * * * * * * * * * 400000 * * * * * * * * * 500000 * * * * * * * * * 600000 * * * * * * * * * 700000 * * * * * * * * * 800000 * * * * * * * * * 900000 * * IOError for file 1433160165278168.json.gz\n",
      "* * * * * * * 1000000 * * * * * * * * * 1100000 * * * * * * * * * 1200000 * * IOError for file 1433472551470078.json.gz\n",
      "* * * * * * * 1300000 * * * * * * * * * 1400000 * * * * * * * * * 1500000 * * * * * * * * * 1600000 * * * * * * * * * 1700000 * * * * * * * * * 1800000 * * * * * * * * * 1900000 * * * * * * * * * 2000000 * * * * IOError for file 1433509491447986.json.gz\n"
     ]
    }
   ],
   "source": [
    "tweet_corpus = load_tweet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1737748\n"
     ]
    }
   ],
   "source": [
    "print len(tweet_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Cleanse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * 100000 * * * * * * * * * 200000 * * * * * * * * * 300000 * * * * * * * * * 400000 * * * * * * * * * 500000 * * * * * * * * * 600000 * * * * * * * * * 700000 * * * * * * * * * 800000 * * * * * * * * * 900000 * * * * * * * * * 1000000 * * * * * * * * * 1100000 * * * * * * * * * 1200000 * * * * * * * * * 1300000 * * * * * * * * * 1400000 * * * * * * * * * 1500000 * * * * * * * * * 1600000 * * * * * * * * * 1700000 * * *\n"
     ]
    }
   ],
   "source": [
    "results, counts = filterTweetCorpus(tweet_corpus, idx_rx_drugs, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets with possible drug references: 1238\n"
     ]
    }
   ],
   "source": [
    "print 'Number of tweets with possible drug references: {}'.format(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample tweets with possible drug references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'drugs': [u'vasopressin'],\n",
      "  'text': u\"one week off the vasopressin; our son's anxiety is back, hitting himself, and the repetitive behavior-all back #asd http://t.co/movz6tp7kw\"},\n",
      " {'drugs': [u'wellbutrin'],\n",
      "  'text': u'@billygunn19 they put me on it to counteract the decrease in hunger from the wellbutrin. thankfully i could use a few pounds :)'},\n",
      " {'drugs': [u'yasmin'],\n",
      "  'text': u'listen to lecture on overcoming sadness and depression - yasmin mogahed by yasmin.mogahed #np on #soundcloud\\nhttp://t.co/kfehqx4yw1'},\n",
      " {'drugs': [u'metformin'],\n",
      "  'text': u'diabetes treatment drug, metformin, could help prevent blindness according #diabetesfree, #diabetes, #stopdiabetes http://t.co/au8xfme5jm'},\n",
      " {'drugs': [u'tamoxifen'],\n",
      "  'text': u'#ovariancyst risk factors: infertility treatment, tamoxifen, pregnancy, hypothyroidism, maternal gonadotropins, cigarettes, tubal ligation'},\n",
      " {'drugs': [u'dexamethasone'],\n",
      "  'text': u're: vaccine treatments for brain tumours: dexamethasone remains critical to control edema, but does this inhibit immune therapy? #asco15'},\n",
      " {'drugs': [u'tramadol'],\n",
      "  'text': u'tramadol prescription pain medication spike emergency room visits http://t.co/cqbpznajmr'},\n",
      " {'drugs': [u'insulin'],\n",
      "  'text': u'quick question! which gains muscle faster? insulin or carbohydrates?\\U0001f60f @ me\\u270c\\ufe0f'},\n",
      " {'drugs': [u'phentermine'],\n",
      "  'text': u'#healthy #nutrition phentermine diet pills for weight loss http://t.co/pxsjttcdvm'},\n",
      " {'drugs': [u'zyrtec'],\n",
      "  'text': u'@jacob_ladder @besafe71 try zyrtec, works the same w/o the drowsiness.'},\n",
      " {'drugs': [u'codeine'],\n",
      "  'text': u'codeine wat am sippin gas wat am inhaling rollin upp dutches damn ma mama yelling chill out y u trippin fukin upp ma section!!!'},\n",
      " {'drugs': [u'phentermine'],\n",
      "  'text': u'#fitbit #exercise phentermine diet pills for weight loss http://t.co/lewmcinmc1 #edsheeran #fatloss'},\n",
      " {'drugs': [u'phentermine'],\n",
      "  'text': u'#solution #fitness phentermine diet pills for weight loss http://t.co/yjb2s58o6e #rays #sports'},\n",
      " {'drugs': [u'phentermine'],\n",
      "  'text': u'#jeremylin #nba phentermine diet pills for weight loss http://t.co/ke7ch258sv #health #workout'},\n",
      " {'drugs': [u'prilosec'],\n",
      "  'text': u'free prilosec sample for heartburn (new link) http://t.co/fr70xvvxp2) http://t.co/9wn1skegde'},\n",
      " {'drugs': [u'sonata'],\n",
      "  'text': u'fit 2006-2010 06-09 10 hyundai sonata wind deflector window visor sun guard 4pc http://t.co/ncuhhmgggj http://t.co/jluvx8d2gx'},\n",
      " {'drugs': [u'promethazine'],\n",
      "  'text': u'promethazine is nausea medicine chill ur not that hard'},\n",
      " {'drugs': [u'yasmin'], 'text': u'@yungtaxi @wolfiehan yasmin chill u hoe :/'},\n",
      " {'drugs': [u'abilify'],\n",
      "  'text': u\"damfuggin abilify commercial... smh glad i got my depression under control. just sayin'. lol! but for real #mentalhealth is serious\"},\n",
      " {'drugs': [u'sonata'],\n",
      "  'text': u\"can't go joy riding cause ain't no gas in the sonata.\"}]\n"
     ]
    }
   ],
   "source": [
    "pprint(results[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of instances where trade name RX drug referenced in tweet containing symptom-related keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'qsymia', 138),\n",
      " (u'saxenda', 111),\n",
      " (u'insulin', 81),\n",
      " (u'codeine', 72),\n",
      " (u'xanax', 67),\n",
      " (u'morphine', 52),\n",
      " (u'methamphetamine', 44),\n",
      " (u'betadine', 31),\n",
      " (u'sonata', 29),\n",
      " (u'orlistat', 21),\n",
      " (u'tramadol', 20),\n",
      " (u'soma', 19),\n",
      " (u'acetaminophen', 17),\n",
      " (u'prozac', 15),\n",
      " (u'ambien', 13),\n",
      " (u'adderall', 13),\n",
      " (u'yaz', 13),\n",
      " (u'norco', 13),\n",
      " (u'valium', 11),\n",
      " (u'oxytocin', 11),\n",
      " (u'cialis', 11),\n",
      " (u'dopamine', 10),\n",
      " (u'phentermine', 10),\n",
      " (u'cortisone', 9),\n",
      " (u'prilosec', 9),\n",
      " (u'promethazine', 9),\n",
      " (u'ortho', 9),\n",
      " (u'yasmin', 8),\n",
      " (u'paxil', 8),\n",
      " (u'hydrocodone', 8),\n",
      " (u'naproxen', 8),\n",
      " (u'glycine', 7),\n",
      " (u'oxycodone', 7),\n",
      " (u'polyethylene', 7),\n",
      " (u'penicillin', 7),\n",
      " (u'metformin', 7),\n",
      " (u'percocet', 7),\n",
      " (u'neosporin', 7),\n",
      " (u'zofran', 7),\n",
      " (u'ativan', 7),\n",
      " (u'zoloft', 6),\n",
      " (u'reglan', 6),\n",
      " (u'nitric', 6),\n",
      " (u'lorazepam', 6),\n",
      " (u'amoxicillin', 6),\n",
      " (u'avc', 6),\n",
      " (u'warfarin', 6),\n",
      " (u'selenium', 5),\n",
      " (u'gabapentin', 5),\n",
      " (u'adrenalin', 5),\n",
      " (u'lithium', 5),\n",
      " (u'celecoxib', 4),\n",
      " (u'doxy', 4),\n",
      " (u'klonopin', 4),\n",
      " (u'omeprazole', 4),\n",
      " (u'niacin', 4),\n",
      " (u'allegra', 4),\n",
      " (u'abilify', 4),\n",
      " (u'seroquel', 4),\n",
      " (u'temozolomide', 4),\n",
      " (u'zyrtec', 4),\n",
      " (u'oxycontin', 4),\n",
      " (u'prometh', 3),\n",
      " (u'doral', 3),\n",
      " (u'ketamine', 3),\n",
      " (u'pce', 3),\n",
      " (u'citalopram', 3),\n",
      " (u'dextromethorphan', 3),\n",
      " (u'progesterone', 3),\n",
      " (u'quinine', 3),\n",
      " (u'mirena', 3),\n",
      " (u'cipro', 3),\n",
      " (u'tranexamic', 3),\n",
      " (u'uloric', 3),\n",
      " (u'allopurinol', 3),\n",
      " (u'celebrex', 3),\n",
      " (u'selsun', 3),\n",
      " (u'fentanyl', 3),\n",
      " (u'diclofenac', 3),\n",
      " (u'albuterol', 3),\n",
      " (u'colchicine', 3),\n",
      " (u'albumin', 3),\n",
      " (u'ventolin', 3),\n",
      " (u'paroxetine', 3),\n",
      " (u'ritalin', 3),\n",
      " (u'nexium', 3),\n",
      " (u'prednisone', 3),\n",
      " (u'clonazepam', 3),\n",
      " (u'sertraline', 2),\n",
      " (u'lyrica', 2),\n",
      " (u'synthroid', 2),\n",
      " (u'xarelto', 2),\n",
      " (u'amitriptyline', 2),\n",
      " (u'evamist', 2),\n",
      " (u'fluoxetine', 2),\n",
      " (u'mpi', 2),\n",
      " (u'plaquenil', 2),\n",
      " (u'zoladex', 2),\n",
      " (u'lasix', 2),\n",
      " (u'tigecycline', 2),\n",
      " (u'sildenafil', 2),\n",
      " (u'hydrocortisone', 2),\n",
      " (u'nuvigil', 2),\n",
      " (u'jalyn', 2),\n",
      " (u'doxepin', 2),\n",
      " (u'glutamine', 2),\n",
      " (u'dexamethasone', 2),\n",
      " (u'cardura', 2),\n",
      " (u'pred', 2),\n",
      " (u'clonidine', 2),\n",
      " (u'doxycycline', 2),\n",
      " (u'cetirizine', 2),\n",
      " (u'naltrexone', 2),\n",
      " (u'voltaren', 2),\n",
      " (u'rivaroxaban', 2),\n",
      " (u'azithromycin', 2),\n",
      " (u'acetylcholine', 2),\n",
      " (u'cymbalta', 2),\n",
      " (u'lexapro', 2),\n",
      " (u'diazepam', 2),\n",
      " (u'zantac', 2),\n",
      " (u'xifaxan', 2),\n",
      " (u'bss', 2),\n",
      " (u'carisoprodol', 2),\n",
      " (u'escitalopram', 1),\n",
      " (u'skelaxin', 1),\n",
      " (u'nandrolone', 1),\n",
      " (u'tamoxifen', 1),\n",
      " (u'epipen', 1),\n",
      " (u'sovaldi', 1),\n",
      " (u'lubiprostone', 1),\n",
      " (u'modafinil', 1),\n",
      " (u'atorvastatin', 1),\n",
      " (u'febuxostat', 1),\n",
      " (u'septra', 1),\n",
      " (u'chlorambucil', 1),\n",
      " (u'midazolam', 1),\n",
      " (u'emend', 1),\n",
      " (u'zithromax', 1),\n",
      " (u'truvada', 1),\n",
      " (u'estradiol', 1),\n",
      " (u'gadavist', 1),\n",
      " (u'pyrazinamide', 1),\n",
      " (u'belviq', 1),\n",
      " (u'lactulose', 1),\n",
      " (u'fioricet', 1),\n",
      " (u'clotrimazole', 1),\n",
      " (u'lamictal', 1),\n",
      " (u'wellbutrin', 1),\n",
      " (u'dabigatran', 1),\n",
      " (u'nalbuphine', 1),\n",
      " (u'naloxone', 1),\n",
      " (u'probenecid', 1),\n",
      " (u'propofol', 1),\n",
      " (u'ospemifene', 1),\n",
      " (u'akynzeo', 1),\n",
      " (u'imodium', 1),\n",
      " (u'albendazole', 1),\n",
      " (u'eszopiclone', 1),\n",
      " (u'aripiprazole', 1),\n",
      " (u'skyla', 1),\n",
      " (u'tamiflu', 1),\n",
      " (u'milrinone', 1),\n",
      " (u'colistin', 1),\n",
      " (u'flagyl', 1),\n",
      " (u'cefotaxime', 1),\n",
      " (u'ipratropium', 1),\n",
      " (u'inderal', 1),\n",
      " (u'dilaudid', 1),\n",
      " (u'abacavir', 1),\n",
      " (u'oxandrolone', 1),\n",
      " (u'ondansetron', 1),\n",
      " (u'nortriptyline', 1),\n",
      " (u'vasopressin', 1),\n",
      " (u'welchol', 1),\n",
      " (u'trazodone', 1),\n",
      " (u'ciprofloxacin', 1),\n",
      " (u'diphenhydramine', 1),\n",
      " (u'tenuate', 1),\n",
      " (u'provigil', 1),\n",
      " (u'mefenamic', 1),\n",
      " (u'tapentadol', 1),\n",
      " (u'miconazole', 1),\n",
      " (u'choline', 1),\n",
      " (u'arginine', 1),\n",
      " (u'azor', 1),\n",
      " (u'ferric', 1),\n",
      " (u'tham', 1),\n",
      " (u'ceftriaxone', 1),\n",
      " (u'methadone', 1),\n",
      " (u'thalidomide', 1),\n",
      " (u'spironolactone', 1),\n",
      " (u'aldara', 1),\n",
      " (u'dextrose', 1),\n",
      " (u'hydrochlorothiazide', 1),\n",
      " (u'chlorhexidine', 1),\n",
      " (u'mirtazapine', 1),\n",
      " (u'lindane', 1),\n",
      " (u'acetic', 1),\n",
      " (u'ultram', 1),\n",
      " (u'osphena', 1),\n",
      " (u'dialyte', 1),\n",
      " (u'opana', 1),\n",
      " (u'isoniazid', 1),\n",
      " (u'lta', 1),\n",
      " (u'ethambutol', 1),\n",
      " (u'simvastatin', 1),\n",
      " (u'isotonic', 1),\n",
      " (u'onexton', 1),\n",
      " (u'methylphenidate', 1),\n",
      " (u'linzess', 1),\n",
      " (u'diflucan', 1),\n",
      " (u'tetracycline', 1),\n",
      " (u'lidocaine', 1),\n",
      " (u'ansaid', 1),\n",
      " (u'premarin', 1),\n",
      " (u'provera', 1),\n",
      " (u'amphetamine', 1),\n",
      " (u'nexplanon', 1),\n",
      " (u'sufenta', 1),\n",
      " (u'risperdal', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted(counts.items(), key=lambda z: z[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( results, open( \"filtered_tweets.p\", \"wb\" ))\n",
    "pickle.dump( counts, open( \"filtered_tweet_counts.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Perform Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulo:4093  Max: 4096\n",
      "*\n",
      "Total duplicate tweets: 517\n",
      "Before: 1238  After: 721\n"
     ]
    }
   ],
   "source": [
    "deduped_tweets = dedupeTweets(results, hash_len=8, hash_bits=24, similarity_threshold=0.6)\n",
    "print 'Before: {0}  After: {1}'.format(len(results),len(deduped_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(deduped_tweets, open( \"deduped_tweets.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
